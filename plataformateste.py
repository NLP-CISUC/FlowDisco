# -*- coding: utf-8 -*-
"""PlataformaTeste.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15-R517fT7u_nxmqt9ICIOSGAQY3QcyTF
"""

# Mount drive - always run 1st 
#from google.colab import drive
#drive.mount("/content/drive", force_remount=True)

# ! git clone https://github.com/YousefGh/kmeans-feature-importance.git
# to use K-Means Feature Importance
#import sys    
#path_to_module = '/content/drive/MyDrive/ColabNotebooks'
#sys.path.append(path_to_module)
# print(sys.path)

# run 1 time
#!pip install --upgrade kneed
#!pip install -qU transformers sentence-transformers
#!pip install clean-text
#!pip install Unidecode

#!pip install kmeans

# IMPORT LIBRARIES AND MODULES
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import gensim
import gensim.corpora as corpora
from gensim.parsing.preprocessing import preprocess_string, strip_punctuation, strip_numeric
import nltk
import spacy
import string
from sklearn import metrics
from sklearn.metrics import accuracy_score
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sentence_transformers import SentenceTransformer
from sklearn.model_selection import ParameterGrid
from nltk import bigrams, trigrams
from sklearn.metrics import davies_bouldin_score
#from kmeans_interp.kmeans_feature_imp import KMeansInterp
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import pairwise_distances_argmin_min
from nltk.stem import WordNetLemmatizer
from transformers import AutoTokenizer, AutoModel
import torch
import sys
from sklearn.manifold import TSNE
from matplotlib import pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from nltk.corpus import stopwords
import networkx as nx
from pprint import pprint 
from graphviz import Source
from networkx.readwrite import text
from io import StringIO
#from kneed import KneeLocator
#import cleantext
import re
import spacy.cli
import getopt
import click

spacy.cli.download("en_core_web_sm")
spacy.cli.download("en_core_web_md")
spacy.cli.download("en_core_web_lg")
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

FOLDER = "." # "/mnt/mydata"

# ---------------------------------------------------------------

#FUNCTIONS

def chooseNumberCluster(opt, normalizedDF):
  column = normalizedDF['trueLabel'].value_counts()

  total = column.sum()
  max_value = column.max()
  min_value = column.min()

  numberExact = normalizedDF.trueLabel.nunique()
  total = total/max_value
  total = round(total, 0)
  total = int(total)

  if opt=="equal":
    nClusters = numberExact
  if opt=="total":
    nClusters = total
  
  #print('O número de Clusters a usar será ' + str(nClusters) + '!!')


  return nClusters

# Normalize turn_id by (all turn_id's/max turn_id) and some column names
def normalizeDataset(csvFileInput, regex=None, removeGreetings=None, speaker=None):
  df_initial = pd.read_csv(csvFileInput, on_bad_lines='skip', sep=";")

  url_pattern = r'https?://\S+'
  url_placeholder = 'xURLx'
  user_tags_pattern = '^@\S+'
  user_tags_placeholder = 'xUSERNAMEx'

  if 'text' in df_initial.columns: 
    df_initial.rename(columns = {'text':'utterance'}, inplace = True)

  if 'Utterance' in df_initial.columns: 
    df_initial.rename(columns = {'Utterance':'utterance'}, inplace = True)

  if 'transcript' in df_initial.columns: 
    df_initial.rename(columns = {'transcript':'utterance'}, inplace = True)

  if 'Msg' in df_initial.columns: 
    df_initial.rename(columns = {'Msg':'utterance'}, inplace = True)

  if 'intent_title' in df_initial.columns: 
    df_initial.rename(columns = {'intent_title':'trueLabel'}, inplace = True)

  if 'dialog_act' in df_initial.columns: 
    df_initial.rename(columns = {'dialog_act':'trueLabel'}, inplace = True)
  
  if 'turn_id' in df_initial.columns: 
    max_value = np.max(df_initial['turn_id'])
    df_initial['turn_id'] = df_initial['turn_id'] / max_value
    df_initial['turn_id'] = df_initial['turn_id'].round(decimals = 3)

  if 'trueLabel' in df_initial.columns:
    df_initial['trueLabel'] = df_initial['trueLabel'].replace(' ', '_', regex=True)

  if 'trueLabel' in df_initial.columns:
    df_initial['utterance']= df_initial['utterance'].apply(lambda x: x.lower())

  if regex is True:
    df_initial['utterance'] = df_initial['utterance'].replace(to_replace = url_pattern, value = url_placeholder, regex = True)
    df_initial['utterance'] = df_initial['utterance'].replace(to_replace = user_tags_pattern, value = user_tags_placeholder, regex = True)

  greetings_stopwords = ["hello", "hi", "bye", "goodbye", "hey"]

  if speaker == "both":
    df_initial = df_initial
    
  if speaker == "0" or speaker == "USER":
    df_initial = df_initial[df_initial['Speaker'].values == 'USR']
    df_initial = df_initial.reset_index(drop=True)

  if speaker == "1" or speaker == "SERVICE":
    df_initial = df_initial[df_initial['Speaker'].values == 'SYS']
    df_initial = df_initial.reset_index(drop=True)
  return df_initial

# Functions needed to use the function to remove Topic Features (Topic Modeling) 
# function to convert sentences to words
def sentencesToWords(sentences): 
    for sentence in sentences:
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True)) # deacc=True -> meaning removes punctuations

# make bigrams
def make_bigrams(texts, dataInput):  
    data = dataInput
    data_words = list(sentencesToWords(data))
    bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)
    bigram_mod = gensim.models.phrases.Phraser(bigram)
    return [bigram_mod[doc] for doc in texts]

# lemmatization, converting a word to its root word. ‘walking’ –> ‘walk’
def lemmatization(nlp, texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent))
        texts_out.append(
            [token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out

# words that represent a topic, can choose in PreProcessing if that words will be ignored
def TopicFeaturesToRemove(nlp, normalizedDF, numberFeatures, TopicFeature=None):
    if TopicFeature is True:  # if it's true ...

        df_initial = normalizedDF
        data = df_initial['utterance'].values.tolist()
        
        # convert sentences to words
        data_words = list(sentencesToWords(data))

        # form bigrams
        data_words_bigrams = make_bigrams(data_words, data)

        # do lemmatization keeping only noun, adj, vb, adv
        data_lemmatized = lemmatization(nlp, data_words_bigrams, allowed_postags=[
            'NOUN', 'ADJ', 'VERB', 'ADV'])

        # Create Dictionary
        id2word = corpora.Dictionary(data_lemmatized)

        # Create Corpus
        texts = data_lemmatized

        # Term Document Frequency
        corpus = [id2word.doc2bow(text) for text in texts]

        lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,
                                                    id2word=id2word,
                                                    num_topics=20,
                                                    random_state=100,
                                                    update_every=1,
                                                    chunksize=100,
                                                    passes=10,
                                                    alpha='auto',
                                                    per_word_topics=True)

        lda_topics = lda_model.show_topics(num_words=numberFeatures)
        
        topics = []
        filters = [lambda x: x.lower(), strip_punctuation, strip_numeric]
        for topic in lda_topics:
            topics.append(preprocess_string(topic[1], filters))
            flat_list = [item for sublist in topics for item in sublist]
            print(flat_list)
            return flat_list
    else:
        flat_list = []
        return flat_list

# function to use the models of sentenceTransformer
def useSentenceTransformer(normalizedDF, model):
    df_initial = normalizedDF
    docs = df_initial['utterance']

    # transform Series into list
    sentences = df_initial['utterance'].tolist()
    
    # sentences are encoded by calling model.encode()
    embeddings = model.encode(sentences)
    #print(embeddings)

    vectors = np.array(embeddings)
    print(vectors.shape)
    #vectors = np.append(vectors, df_initial['turn_id'], axis=2)
    #final_vectors = np.column_stack((vectors,df_initial['turn_id'], df_initial['interlocutor']))
    #final_vectors = np.column_stack(vectors)

    #df_final = pd.DataFrame(final_vectors) #doubt if the return is final_vectors or df_final
    #print(final_vectors)
    print("--> sentence-transformers used with success!")
    #pd.DataFrame(final_vectors).to_csv(r'/content/drive/MyDrive/ColabNotebooks/datasets/DFProcessed.csv', index=False)
    #final_vectors = pd.concat((vectors, df_initial['turn_id'], df_initial['interlocutor']), axis = 1).values #axis 1 meaning concatenates in column wise

    return vectors

# function to convert word to vectors
def word2vec(nlp, normalizedDF):
  df_initial = normalizedDF

  X = [d.vector for t in df_initial['utterance'] for d in nlp(str(t))]
  vectors = np.array(X)

  print(vectors.shape)
  print("--> word2vec used with success!")
  return vectors

# function to use TD-IDF 
def PreProcessingTFIDF(stopwordseng, normalizedDF, topicFeatures, maxDF, minDF):
    df_initial = normalizedDF
    df_final = pd.DataFrame()

    # turn 'utterance' column into list and all lowercase
    df_initial['utterance'].tolist()
    df_initial['utterance'] = [
        x for x in df_initial['utterance'].map(lambda x: str(x).lower())]

    stopwordsAndTopic = stopwordseng
    stopwordsAndTopic.extend(topicFeatures)  # Stopwords + Topic Features
    
    # TfidfVectorizer
    count_vectTFIDF = TfidfVectorizer(
        stop_words=stopwordsAndTopic, max_df=maxDF, min_df=minDF, ngram_range=(1, 1))
    count_matrix = count_vectTFIDF.fit_transform(
        df_initial['utterance'].tolist())
    count_array = count_matrix.toarray()
    df_TFIDF = pd.DataFrame(
        data=count_array, columns=count_vectTFIDF.get_feature_names_out())
    
    # guardar dataframes num csv
    final_vectors = df_TFIDF #axis 1 meaning concatenates in column wise
    return final_vectors

# function that receive vectors and do the clustering in KMeans
def ClusteringKMeans(vectors, nClusters):
    

    kmeans = KMeans(n_clusters=nClusters, init='k-means++', max_iter=500)
    kmeans.fit(vectors)
    labelsKMeans = kmeans.labels_
    centersKMeans = kmeans.cluster_centers_
    print("\nInternal Evaluation:\nSilhouette Score: ", metrics.silhouette_score(vectors, labelsKMeans))
    print("Davies–Bouldin Index (DBI): ", davies_bouldin_score(vectors, labelsKMeans))
    
    return labelsKMeans, centersKMeans

# function that receive vectors and do the clustering in DBSCAN (-1 cluster represents outliers)
def ClusteringDBSCAN(vectors, eps, minsamples):

    dbscan = DBSCAN(eps = eps, min_samples = minsamples).fit(vectors) # fitting the model
    labelsDBSCAN = dbscan.labels_ # getting the labels
    print(labelsDBSCAN)
    
    nClusters = np.unique(dbscan.labels_).size
    print("\nInternal Evaluation:\nSilhouette Score: ", metrics.silhouette_score(vectors, labelsDBSCAN))
    print("Davies–Bouldin Index (DBI): ", davies_bouldin_score(vectors, labelsDBSCAN))

    return labelsDBSCAN

def setLabels(normalizedDF, problem):  # change de setLabels() according to the problem to be treated

    normalizedDF['trueLabel'] = normalizedDF['trueLabel'].fillna("none")
    normalizedDF['trueLabel'] = normalizedDF['trueLabel'].astype(str)
    total = normalizedDF['trueLabel'].value_counts().sum()
    print(normalizedDF['trueLabel'].value_counts())
    setOfLabels = set()
    
    if problem == "multi":
      print("Multi-Label Problem")
      for i in range(len(normalizedDF['trueLabel'])):
          for j in normalizedDF['trueLabel'][i].split('\n'):
            setOfLabels.add(j.split(",")[0])
    if problem == "single":
      print("Single-Label Problem")
      for i in range(len(normalizedDF['trueLabel'])):
        for j in normalizedDF['trueLabel'][i].split('\n'):
          setOfLabels.add(j.split(" ")[0])
    
    print(setOfLabels)
    return setOfLabels

def Evaluation(y_predicted, normalizedDF, setOfLabels, nClusters):
    evaluationDF = pd.DataFrame() 

    evaluationDF['trueLabel'] = normalizedDF['trueLabel'].fillna("none")

    labels_pred = np.array(y_predicted)
    
    ground_truth = pd.DataFrame(evaluationDF, columns=['trueLabel'])

    mapOfLabels = dict(zip(setOfLabels, np.arange(nClusters)))

    labels_true = evaluationDF['trueLabel'].map(mapOfLabels)

    counts = np.linspace(0, 0, num=nClusters*nClusters).reshape(
        (nClusters, nClusters))  # columns clusters, lines labels
    
    for i in range(len(labels_pred)):
        for j in ground_truth['trueLabel'][i].split(';'):
            if j.split(" ")[0] != 'noofferr':
                counts[mapOfLabels[j.split(",")[0]]][int(
                    labels_pred[i])] = counts[mapOfLabels[j.split(",")[0]]][int(labels_pred[i])] + 1
    print("\n")
    print(counts)
    for i in range(nClusters):  # cluster i
        maior = 0
        indice = -1
        for j in range(nClusters):  # label j
            if counts[j][i] >= maior:
                maior = counts[j][i]
                indice = j
        if indice != -1:
            print('cluster:', i, 'label:', list(
                setOfLabels)[indice])
            evaluationDF['trueLabel'] = evaluationDF['trueLabel'].replace([list(
                setOfLabels)[indice]], [i])
        else:
            print('cluster:', i, 'label:not defined')

    print("\nExternal Evaluation:\nAccuracy: ", accuracy_score(labels_true, labels_pred))
    print('V-Measure: ', metrics.v_measure_score(labels_true, labels_pred))

def createPCA(y_predicted, normalizedDF, vectors, nClusters):
  
  df_initial = normalizedDF
  y_true = df_initial['trueLabel']

  Sc = StandardScaler()
  X = Sc.fit_transform(vectors)

  pca = PCA(n_components=3)
  pca_result = pca.fit_transform(X)

  # sns settings
  sns.set(rc={'figure.figsize':(13,9)})

  # colors
  palette = sns.hls_palette(nClusters, l=.4, s=.9)

  # plot
  sns.scatterplot(x=pca_result[:,0], y=pca_result[:,1], hue=y_true, legend='full', palette=palette)
  plt.title('PCA with Kmeans Labels')
  #plt.savefig("improved_cluster_tsne.png")
  plt.show()

def createTSNE(y_predicted, normalizedDF, vectors, nClusters):
  
  df_initial = normalizedDF
  df_initial = df_initial.rename(columns={'trueLabel': 'Intents'}) # Or Dialog Acts
  y_true = df_initial['Intents'] # Or Dialog Acts

  Sc = StandardScaler()
  X = Sc.fit_transform(vectors)


  tsne = TSNE(verbose=1, perplexity=50, learning_rate='auto', init='random')  # Changed perplexity from 100 to 50 per FAQ
  X_embedded = tsne.fit_transform(X)

  # sns settings
  sns.set(rc={'figure.figsize':(13,9)})

  # colors
  palette = sns.hls_palette(nClusters, l=.4, s=.9)

  # plot
  sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=y_true, legend='full', palette=palette)
  plt.title('t-SNE with Kmeans Labels')
  #plt.savefig("improved_cluster_tsne.png")
  plt.show()

def transition_matrix(nClusters, transitions): #transitions is for y_predicted
    n = nClusters #number of states

    M = [[0]*n for _ in range(n)]

    for (i,j) in zip(transitions,transitions[1:]):
        M[i][j] += 1

    #now convert to probabilities:
    for row in M:
        s = sum(row)
        if s > 0:
            row[:] = [f/s for f in row]
    return M

def createMatrix(nClusters, clusLabelled, normalizedDF, y_predicted): #return of matrix
  df_initial = normalizedDF
  df_probs = pd.DataFrame() 
  df_probs['predicted'] = y_predicted

  matriz = np.zeros(shape=(nClusters, nClusters))
  t = df_probs['predicted']
  m = transition_matrix(nClusters, t)
  names = [_ for _ in clusLabelled['labels']]
  df_matrix = pd.DataFrame(m, index=names, columns=names)
  df_matrix = df_matrix.round(decimals = 2)

  return df_matrix

def generateMarkovChain(nClusters, matrix):

  states = list(matrix.columns.values)
  # equals transition probability matrix of changing states given a state
  q_df = pd.DataFrame(columns=states, index=states)
  q_df = matrix
  
  for p in range(nClusters):
    q_df.loc[states[p]].reset_index = matrix.iloc[p]

  def _get_markov_edges(Q):
    edges = {}
    for col in Q.columns:
        for idx in Q.index:
            edges[(idx,col)] = Q.loc[idx,col]
    return edges
  

  edges_wts = _get_markov_edges(q_df)
  #pprint(edges_wts)
  # create graph object
  G = nx.MultiDiGraph()
 

  # nodes correspond to states
  G.add_nodes_from(states)
  
  # edges represent transition probabilities
  for k, v in edges_wts.items():
      tmp_origin, tmp_destination = k[0], k[1]
      G.add_edge(tmp_origin, tmp_destination, weight=v, label=v)


  #remove edges below threshold
  threshold = 0.2
  G.remove_edges_from([(n1, n2) for n1, n2, w in G.edges(data="weight") if w < threshold])
  # nx.drawing.nx_pydot.write_dot(G, f'{FOLDER}/markov.dot')

def silhouetteMethod(vectors, minK, maxK, incr):
  data = vectors
  numberClusters = 0
  atual_silhouette = 0.000
  #Prepare the scaler
  scale = StandardScaler().fit(data)

  #Fit the scaler
  scaled_data = pd.DataFrame(scale.fit_transform(data))
  #scaled_data = pd.DataFrame(scale.fit_transform(data),columns = data.columns, index = data.index)
  X = scaled_data

  #For the silhouette method k needs to start from 2
  K = range(minK, maxK, incr)
  silhouettes = []
  
  #Fit the method
  for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, init='random')
    kmeans.fit(X)
    silhouettes.append(silhouette_score(X, kmeans.labels_))
    score = silhouette_score(X, kmeans.labels_)
    print(score)
    if score > atual_silhouette:
      numberClusters = k
      atual_silhouette = score
  #Plot the results
  fig = plt.figure(figsize= (15,5))
  plt.plot(K, silhouettes, 'bx-')
  plt.xlabel('Values of K')
  plt.ylabel('Silhouette score')
  plt.title('Silhouette Method')
  plt.grid(True)
  plt.show()

  #return kl.elbow
  return numberClusters

def describeClustersBigrams(nClusters, normalizedDF, y_predicted):
  df_initial = normalizedDF

  df_teste = pd.DataFrame() # utterance and y_predicted
  df_teste['predicted'] = y_predicted
  df_teste['corpus'] = df_initial['utterance']

  LC=[] #clusters
  LBG=[] #bigrams

  for p in range(nClusters):
    predictedDF = df_teste[df_teste['predicted'] == p]  #mudar numeros
    corpus = predictedDF['corpus']

    df_corpus = pd.DataFrame(corpus)
    df_corpus.columns = ['docs_in_cluster']

    c_vec = CountVectorizer(stop_words=None, ngram_range=(2,2))

    # matrix of ngrams
    ngrams = c_vec.fit_transform(df_corpus['docs_in_cluster'].astype('U'))

    # count frequency of ngrams
    count_values = ngrams.toarray().sum(axis=0)

    # list of ngrams
    vocab = c_vec.vocabulary_
    
    df_ngram = pd.DataFrame(sorted([(count_values[i],k) for k,i in vocab.items()], reverse=True)
                ).rename(columns={0: 'frequency', 1:'labels'})
    LC.append(f'Cluster {p}')
    LBG.append(df_ngram['labels'].iat[0])

  df1 = pd.DataFrame(LC, columns=['clusters'])
  df2 = pd.DataFrame(LBG, columns=['labels'])
  df2['labels'] = df2['labels'].str.cat(df2.groupby("labels").cumcount().astype(str).str.replace("0", ""), sep = " ") # handle repeated labelled
  
  df_labels = pd.concat([df1, df2], axis=1)
  print("Cluster Describe Bigrams:")

  return df_labels

def describeClustersVerbs(nlp, nClusters, normalizedDF, y_predicted):
  df_initial = normalizedDF

  df_teste = pd.DataFrame() # utterance and y_predicted
  df_teste['predicted'] = y_predicted
  df_teste['corpus'] = df_initial['utterance']
  df_teste['corpus'] = df_teste['corpus'].astype(str) #?
  df_verbs = pd.DataFrame()
  LC=[] #clusters
  LV=[] #verbs

  listSintagmas=[]

  for utt in list(df_teste['corpus']):
    doc = nlp(utt)
    sintagma = ' ' 
    for token in doc: 
      if token.text[0] >= 'A' and token.text[0] <= 'z': 
        if token.dep_ == 'ROOT': 
          sintagma = sintagma + " " + token.lemma_ 
          for child in token.children: 
            if child.text[0] >= 'A' and child.text[0] <= 'z' and child.dep_ != 'nsubj': 
              sintagma = sintagma + " " + child.lemma_ 
    listSintagmas.append(sintagma)

  df_teste['corpus'] = listSintagmas
  
  #remove white space, with this verbs technic happen to much
  df_teste['corpus'].replace(' ', np.nan, inplace=True)
  df_teste.dropna()
  
  for p in range(nClusters):
    
    predictedDF = df_teste[df_teste['predicted'] == p]  #mudar numeros

    corpus = predictedDF['corpus']

    df_corpus = pd.DataFrame(corpus)
    df_corpus.columns = ['docs_in_cluster']

    # count frequency of verbs
    count_values = df_corpus.value_counts().index.tolist()[0]
    

    LC.append(f'Cluster {p}')
    LV.append(count_values[0])

  df1 = pd.DataFrame(LC, columns=['clusters']) 
  df2 = pd.DataFrame(LV, columns=['labels'])
  df2['labels'] = df2['labels'].str.cat(df2.groupby("labels").cumcount().astype(str).str.replace("0", ""), sep = " ") # handle repeated labelled
  
  df_labels = pd.concat([df1, df2], axis=1)

  print("Cluster Describe Verbs:")
  
  return df_labels

def describeClustersClosest(nClusters, normalizedDF, y_predicted, vectors, centers):

  df_initial = normalizedDF
  docs = df_initial['utterance']
  order_centroids = centers.argsort()[:, ::-1]
  closest, _ = pairwise_distances_argmin_min(centers, vectors, metric='cosine')
  mydict = {i: np.where(y_predicted == i)[0] for i in range(len(set(y_predicted)))}


  df_teste = pd.DataFrame() # utterance and y_predicted
  df_teste['predicted'] = y_predicted
  df_teste['corpus'] = df_initial['utterance']

  LC=[] #clusters
  LCD=[] #closest document


  for p in range(nClusters):
    
    predictedDF = df_teste[df_teste['predicted'] == p]  #mudar numeros
    corpus = predictedDF['corpus']

    df_corpus = pd.DataFrame(corpus)
    df_corpus.columns = ['docs_in_cluster']

    # count frequency of verbs
    count_values = df_corpus.value_counts().index.tolist()[0]

    LC.append(f'Cluster {p}')
    LCD.append(docs[closest[p]])

  df1 = pd.DataFrame(LC, columns=['clusters']) 
  df2 = pd.DataFrame(LCD, columns=['labels'])
  df2['labels'] = df2['labels'].str.cat(df2.groupby("labels").cumcount().astype(str).str.replace("0", ""), sep = " ") # handle repeated labelled
  
  df_labels = pd.concat([df1, df2], axis=1)
  
  print("Cluster Describe Closest Document:")
  return df_labels


# MAIN FUNCTION
def main(package, representation, labels_type, n_clusters):
    nlp = spacy.load(package)
    stopwordseng = nltk.corpus.stopwords.words('english')

    # Models of SentenceTransformer choosed by results here: 'https://www.sbert.net/docs/pretrained_models.html'
    #mpnetBase = SentenceTransformer('all-mpnet-base-v2')
    #distilroberta = SentenceTransformer('all-distilroberta-v1')
    #miniLM12 = SentenceTransformer('all-MiniLM-L12-v2')
    miniLM6 = SentenceTransformer('all-MiniLM-L6-v2')
    problem = "single"
    csvFileInput = f"{FOLDER}/twitter_full_dataset-pequeno.csv"

    normalizedDF = normalizeDataset(csvFileInput, regex=True, removeGreetings=False, speaker=False) #normalize turn_id, regex if true normalize URL's and Usernames started with '@', remove greeting words
    TopicFeatures = TopicFeaturesToRemove(nlp, normalizedDF, numberFeatures, TopicFeature=False) #topic features

    #só caso exista labels 
    if 'trueLabel' in normalizedDF.columns: 
      nDAsInt = normalizedDF['trueLabel'].nunique() #nº de Dialog Acts / Intents reais -> usado para o t-SNE e PCA
      setOfLabels = setLabels(normalizedDF, problem) #return of the labels

    # Functions to transform into vectors (WORD EMBEDDING)
    model = miniLM6 # mpnetBase / distilroberta / miniLM12 / miniLM6

    if representation == 'tfidf':
      vectors = PreProcessingTFIDF(stopwordseng,normalizedDF, TopicFeatures, 0.8, 3)
    elif representation == 'word2vec':
      vectors = word2vec(normalizedDF) 
    else:
      vectors = useSentenceTransformer(normalizedDF, model)

    #K-Means
    if 'trueLabel' in normalizedDF.columns:  #supervised way, if there the column with labels
      nClusters = chooseNumberCluster("equal", normalizedDF) #equal | total # nCluster = nDA / Int | total / max
    else:
      nClusters = silhouetteMethod(vectors, 2, 14, 1) #unsupervised way (vectors, minK, maxK, increment) sequence of numbers from minK to maxK, but increment
    print('O número de Clusters a usar será ' + str(nClusters) + '!!')
    
    #Colocado de forma manual porque 2 clusters era pouco
    if n_clusters:
      nClusters = n_clusters

    #K-Means
    y_predicted, centers = ClusteringKMeans(vectors, nClusters)

    #só caso exista labels 
    if 'trueLabel' in normalizedDF.columns: 
      Evaluation(y_predicted, normalizedDF, setOfLabels, nClusters)

    # Describe Clusters -> Bigrams | Verbs | Closest Document
    bigrams = describeClustersBigrams(nClusters, normalizedDF, y_predicted) #return dataframe with n_clusters and respective label (bigram)
    print(bigrams)

    verbs = describeClustersVerbs(nlp, nClusters, normalizedDF, y_predicted) #return dataframe with n_clusters and respective label (verbs)
    print(verbs)

    closestDocuments = describeClustersClosest(nClusters, normalizedDF, y_predicted, vectors, centers)
    print(closestDocuments)

    if labels_type == 'verbs':
      matrix = createMatrix(nClusters, verbs, csvFileInput, y_predicted) #change first position according the way we wanna describe the clusters
    elif labels_type == 'closestDocuments':
      matrix = createMatrix(nClusters, closestDocuments, csvFileInput, y_predicted) #change first position according the way we wanna describe the clusters
    else:
      matrix = createMatrix(nClusters, bigrams, csvFileInput, y_predicted) #change first position according the way we wanna describe the clusters

    generateMarkovChain(nClusters,matrix)

